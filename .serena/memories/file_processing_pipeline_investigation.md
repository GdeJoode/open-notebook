# Open Notebook File Processing Pipeline - Complete Investigation

## EXECUTIVE SUMMARY

The open-notebook application has a sophisticated, multi-layered file processing architecture:
1. **Frontend**: Multi-step file upload dialog with notebook selection
2. **API Layer**: Multipart form-based file upload with async/sync processing modes
3. **Processing**: LangGraph-based orchestration with content extraction, chunking, and optional embedding
4. **Storage**: SurrealDB for all persistent data; local filesystem for uploaded files
5. **Monitoring**: surreal-commands job queue with status tracking

---

## 1. FILE INPUT SYSTEM

### 1.1 Frontend File Upload Interface

**Location**: `frontend/src/components/sources/AddSourceDialog.tsx`

**Features**:
- 3-step wizard: Source Type → Organization → Processing
- Supports 3 source types: `upload`, `link`, `text`
- File upload uses HTML5 FileList API
- Async processing flag: default `true`
- Optional notebook selection (multi-select)
- Optional transformation selection

**Dialog State**:
```typescript
type CreateSourceFormData = {
  type: 'link' | 'upload' | 'text'
  title?: string
  url?: string  // For links
  content?: string  // For text sources
  file?: File  // For uploads
  notebooks?: string[]
  transformations?: string[]
  embed: boolean
  async_processing: boolean
}
```

**Form Validation** (Zod schema):
- Upload type requires File
- Link type requires non-empty URL
- Text type requires non-empty content AND title
- Embed and async_processing default to boolean values

### 1.2 API Upload Endpoint

**Location**: `api/routers/sources.py:create_source()` (lines 319-586)

**Endpoint**: `POST /sources`

**Request Format**: `multipart/form-data`

**Form Fields**:
- `type`: Required, one of: "link", "upload", "text"
- `notebook_id`: Optional, single notebook (backward compat)
- `notebooks`: Optional, JSON string of notebook IDs
- `url`: Optional, URL for link type
- `content`: Optional, text for text type
- `title`: Optional, source title
- `file`: Optional, uploaded file
- `transformations`: Optional, JSON string of transformation IDs
- `embed`: String "true"/"false" (converted to boolean)
- `delete_source`: String "true"/"false" (auto-delete after processing)
- `async_processing`: String "true"/"false"

**Processing Mode**:
```
async_processing=true  → Returns immediately, queues background job, status trackable
async_processing=false → Blocks until complete, returns full result
```

### 1.3 File Storage

**Location**: `open_notebook/config.py`

**Upload Directory**: `./data/uploads/`

**File Naming**:
- Unique naming: "filename (1).pdf", "filename (2).pdf", etc.
- Generated by `generate_unique_filename()` function
- Prevents overwrite of existing files

**File Lifecycle**:
1. Uploaded to `UPLOADS_FOLDER`
2. Path stored in Source.asset.file_path
3. Optional: Auto-delete after processing (`auto_delete_files` setting)
4. Available for download via `/sources/{id}/download`

---

## 2. FILE PROCESSING PIPELINE

### 2.1 Processing Flow Diagram

```
POST /sources (file upload)
    ↓
[save_uploaded_file] → file saved to ./data/uploads/
    ↓
[parse_source_form_data] → Create SourceCreate model
    ↓
Branch: async vs sync processing
    ↓
[CREATE SOURCE RECORD] → Save minimal source to DB
    ↓
ASYNC: Queue command
SYNC: Execute command directly
    ↓
[process_source_command] (commands/source_commands.py)
    ↓
[source_graph.ainvoke] (open_notebook/graphs/source.py)
    ↓
[content_process] → Extract content + chunks
    ↓
[save_source] → Store extracted data
    ↓
[transform_content] (optional) → Apply transformations
```

### 2.2 LangGraph Workflow (source_graph)

**Location**: `open_notebook/graphs/source.py`

**Graph State** (SourceState TypedDict):
```python
{
    'content_state': ProcessSourceState,  # From content-core
    'apply_transformations': List[Transformation],
    'source_id': str,
    'notebook_ids': List[str],
    'source': Source,  # Database object
    'transformation': Annotated[list, operator.add],  # Accumulator
    'embed': bool,
    'chunks': Optional[List[Dict[str, Any]]]  # Extracted chunks
}
```

**Nodes**:
1. `content_process` (lines 35-91)
2. `save_source` (lines 94-163)
3. `transform_content` (lines 185-204)

**Edges**:
```
START → content_process → save_source → [conditional] transform_content → END
```

### 2.3 Content Extraction (content_process node)

**Engine Selection** (lines 49-61):
```python
document_engine = content_settings.default_content_processing_engine_doc
if document_engine == "docling_gpu":
    processed_state = await extract_with_docling_gpu(content_state)
else:
    processed_state = await extract_content(content_state)
```

**Supported Engines**:
- `docling_gpu`: GPU-accelerated (8-14x faster)
- `docling`: CPU-only
- `auto`: content-core decides
- `simple`: Basic extraction

**Content State Input**:
```python
{
    'file_path': str,  # For uploads
    'url': str,  # For links
    'content': str,  # For text sources
    'delete_source': bool,
    'document_engine': str,
    'url_engine': str,
    'output_format': 'markdown'  # Hardcoded
}
```

**Output** (ProcessSourceState):
```python
{
    'content': str,  # Extracted text
    'file_path': str,  # Preserved from input
    'url': str,  # Preserved from input
    'title': str,  # Extracted or inferred
    'metadata': {
        'extraction_engine': str,
        'docling_format': str,
        'docling_gpu_enabled': bool,
        # ... engine-specific fields
    }
}
```

### 2.4 Chunk Extraction

**Location**: `open_notebook/processors/chunk_extractor.py`

**Trigger**: PDF files or docling-processed documents (lines 72-86)

**Process**:
1. Re-invoke Docling to extract chunks with spatial data
2. Iterate through document items (TextItem, TableItem, PictureItem)
3. Extract bounding boxes from provenance data
4. Track chapters, sections, paragraph numbers
5. Return chunk list with positions

**Chunk Data Structure**:
```python
{
    'text': str,
    'order': int,  # Position in document
    'physical_page': int,  # PDF page number (0-indexed)
    'printed_page': int | None,  # Print edition page number
    'chapter': str | None,  # Section heading
    'paragraph_number': int | None,  # Within section
    'element_type': str,  # 'paragraph', 'title', 'table', 'picture'
    'positions': [[page, x1, x2, y1, y2], ...],  # Bounding boxes
    'metadata': {
        'has_spatial_data': bool,
        'num_locations': int,
        'item_type': str
    }
}
```

**Bounding Box Format**:
- `[page_number, x1, x2, y1, y2]`
- Coordinates normalized to 0-1 range (percentage of page)
- Multiple positions for items spanning pages/columns

### 2.5 Source Saving (save_source node)

**Location**: `open_notebook/graphs/source.py:save_source()` (lines 94-163)

**Steps**:
1. Load existing Source record by ID
2. Update asset (file_path or url)
3. Set full_text to extracted content
4. Optionally update title
5. Save source to database
6. Save chunks if extracted (idempotent - deletes old, creates new)
7. Optionally vectorize for embeddings

**Chunk Persistence** (lines 118-154):
- Chunks stored in `chunk` table
- Each chunk has: source_id, text, order, page info, positions, metadata
- Idempotent: old chunks deleted before new ones saved
- Failures don't break source saving

### 2.6 Command Queue System

**Location**: `commands/source_commands.py`

**Command Class** (lines 30-45):
```python
class SourceProcessingInput(CommandInput):
    source_id: str
    content_state: Dict[str, Any]
    notebook_ids: List[str]
    transformations: List[str]
    embed: bool

class SourceProcessingOutput(CommandOutput):
    success: bool
    source_id: str
    embedded_chunks: int
    insights_created: int
    processing_time: float
    error_message: Optional[str]
```

**Async Processing Path** (api/routers/sources.py:389-445):
1. Create source record immediately (title="Processing...")
2. Add to notebooks immediately (for UI responsiveness)
3. Submit command to surreal-commands
4. Return source with command_id and status="queued"
5. Frontend polls `/sources/{id}/status` for updates

**Sync Processing Path** (api/routers/sources.py:464-550):
1. Create source record
2. Add to notebooks
3. execute_command_sync() - blocks until complete (300s timeout)
4. Check result status
5. Return fully processed source

---

## 3. FRONTEND COMPONENTS

### 3.1 Add Source Dialog

**File**: `frontend/src/components/sources/AddSourceDialog.tsx`

**Child Components**:
- `SourceTypeStep`: Select upload/link/text
- `NotebooksStep`: Multi-select notebooks
- `ProcessingStep`: Select transformations, embedding option

**State Management**:
- Current step tracking
- Form state via react-hook-form + Zod
- Processing status monitoring
- Timeout cleanup

### 3.2 Source Type Step

**File**: `frontend/src/components/sources/steps/SourceTypeStep.tsx`

**Handles**:
- File upload UI (uses native file input)
- URL input validation
- Text content textarea
- Title input

### 3.3 Add Existing Source Dialog

**File**: `frontend/src/components/sources/AddExistingSourceDialog.tsx`

**Purpose**: Associate existing sources with notebooks

### 3.4 Source Detail Components

**Files**:
- `SourceDetailContent.tsx`: Display source details
- `PdfChunkViewer.tsx`: PDF visualization with bounding boxes
- `SourceInsightDialog.tsx`: Transformation results
- `ChatPanel.tsx`: Chat interface for source

---

## 4. DATA MODELS & DATABASE

### 4.1 Source Domain Model

**Location**: `open_notebook/domain/notebook.py:Source` (lines 197-206)

**Fields**:
```python
class Source(ObjectModel):
    table_name = "source"
    asset: Optional[Asset]  # file_path, url
    title: Optional[str]
    topics: Optional[List[str]]
    full_text: Optional[str]  # Extracted content
    command: Optional[RecordID]  # Link to command job
```

**Methods**:
- `get_status()`: Get command status
- `get_processing_progress()`: Get detailed progress
- `get_chunks()`: Get associated chunks
- `get_embedded_chunks()`: Count embedded chunks
- `get_insights()`: Get transformation results
- `vectorize()`: Create embeddings

### 4.2 Chunk Domain Model

**Location**: `open_notebook/domain/notebook.py:Chunk` (lines 143-194)

**Fields**:
```python
class Chunk(ObjectModel):
    table_name = "chunk"
    source: Union[str, RecordID]  # Foreign key
    text: str
    order: int
    physical_page: int
    printed_page: Optional[int]
    chapter: Optional[str]
    paragraph_number: Optional[int]
    element_type: str
    positions: List[List[float]]  # [[page, x1, x2, y1, y2], ...]
    metadata: Optional[Dict[str, Any]]
```

### 4.3 Asset Model

**Location**: `open_notebook/domain/notebook.py:Asset` (lines 88-90)

**Fields**:
```python
class Asset(BaseModel):
    file_path: Optional[str]
    url: Optional[str]
```

### 4.4 Database Tables

**SurrealDB Tables**:
- `source`: Main source records
- `chunk`: Document chunks with spatial data
- `source_embedding`: Vector embeddings
- `source_insight`: Transformation results
- `reference`: Notebook-source associations
- `command`: Command queue (surreal-commands)

### 4.5 API Response Models

**Location**: `api/models.py`

```python
class SourceCreate(BaseModel):
    type: str  # 'link', 'upload', 'text'
    notebooks: Optional[List[str]]
    url: Optional[str]
    file_path: Optional[str]
    content: Optional[str]
    title: Optional[str]
    transformations: Optional[List[str]]
    embed: bool
    delete_source: bool
    async_processing: bool

class SourceResponse(BaseModel):
    id: str
    title: Optional[str]
    topics: Optional[List[str]]
    asset: Optional[AssetModel]
    full_text: Optional[str]
    embedded: bool
    embedded_chunks: int
    created: str
    updated: str
    command_id: Optional[str]  # For async processing
    status: Optional[str]  # 'queued', 'running', 'completed', 'failed'
    processing_info: Optional[Dict]  # starttime, endtime, error

class SourceListResponse(BaseModel):
    id: str
    title: Optional[str]
    topics: Optional[List[str]]
    asset: Optional[AssetModel]
    embedded: bool
    embedded_chunks: int
    insights_count: int
    created: str
    updated: str
    command_id: Optional[str]
    status: Optional[str]
    processing_info: Optional[Dict]
```

---

## 5. API ENDPOINTS

### 5.1 Source Management

```
POST /sources
  - Create new source (upload, link, or text)
  - Form data: type, file?, url?, content?, title?, notebooks?, transformations?, embed, async_processing
  - Returns: SourceResponse with command_id if async

GET /sources
  - List sources with filtering/pagination
  - Query params: notebook_id?, limit, offset, sort_by, sort_order
  - Returns: List[SourceListResponse]

GET /sources/{id}
  - Get source details
  - Returns: SourceResponse with status info

PUT /sources/{id}
  - Update source (title, topics)
  - Returns: SourceResponse

DELETE /sources/{id}
  - Delete source and associated data

POST /sources/{id}/retry
  - Retry failed processing
  - Returns: SourceResponse with new command_id

GET /sources/{id}/status
  - Get detailed processing status
  - Returns: SourceStatusResponse

GET /sources/{id}/download
  - Download original uploaded file
  - Returns: File binary

HEAD /sources/{id}/pdf
GET /sources/{id}/pdf
  - Get PDF for visualization
  - Returns: PDF file (inline, supports range requests)

GET /sources/{id}/chunks
  - Get chunks with spatial data
  - Returns: {chunks, total_chunks, has_spatial_data}

GET /sources/{id}/insights
  - Get transformation results
  - Returns: List[SourceInsightResponse]

POST /sources/{id}/insights
  - Run transformation on source
  - Returns: SourceInsightResponse
```

### 5.2 Settings

```
GET /settings
  - Get current ContentSettings
  - Returns: ContentSettings

PUT /settings
  - Update ContentSettings
  - Body: {default_content_processing_engine_doc?, ...}
  - Returns: Updated ContentSettings
```

---

## 6. CONFIGURATION SYSTEM

### 6.1 Content Settings

**Location**: `open_notebook/domain/content_settings.py`

**Fields**:
```python
class ContentSettings(RecordModel):
    record_id = "open_notebook:content_settings"  # Singleton record
    default_content_processing_engine_doc: Literal["auto", "docling", "docling_gpu", "simple"]
    default_content_processing_engine_url: Literal["auto", "firecrawl", "jina", "simple"]
    default_embedding_option: Literal["ask", "always", "never"]
    auto_delete_files: Literal["yes", "no"]
    youtube_preferred_languages: List[str]
```

**Storage**: SurrealDB single record with record_id "open_notebook:content_settings"

**Frontend Settings UI**: `frontend/src/app/(dashboard)/settings/components/SettingsForm.tsx`

---

## 7. DOWNSTREAM PROCESSING

### 7.1 Embedding Pipeline

**Trigger**: If `embed=true` in source processing

**Process**: `Source.vectorize()` method
- Creates embeddings for full source
- Creates embeddings for chunks if available
- Stores in `source_embedding` table
- Enables vector search

### 7.2 Transformation Pipeline

**Trigger**: If transformations specified in SourceCreate

**Process** (lines 166-204 in source.py):
1. Load Transformation objects by ID
2. For each transformation:
   - Run transformation graph on full_text
   - Store result as SourceInsight
3. Transformation results linked to source

### 7.3 Graph Integration

**Location**: `open_notebook/graphs/source.py:save_source()`

**Process**:
- Source stored with full_text
- Chunks stored with spatial positions
- Vectorization optional (if embed=true)
- Transformations conditional (if provided)

---

## 8. FILE DOWNLOAD & SERVING

### 8.1 Download Endpoint

**Location**: `api/routers/sources.py:download_source_file()` (lines 707-721)

**Security**:
- Resolves path to prevent directory traversal
- Restricts to UPLOADS_FOLDER
- Returns 403 if outside uploads directory

**Returns**: File binary with correct media type

### 8.2 PDF Serving

**Location**: `api/routers/sources.py:get_source_pdf()` (lines 1099-1153)

**Features**:
- Supports GET and HEAD methods (for PDF.js)
- Inline disposition (displays in browser, not download)
- Includes range headers (for seek/scroll in viewer)
- CORS headers for PDF.js cross-domain access

**Returns**: PDF file with:
- `Content-Disposition: inline`
- `Accept-Ranges: bytes`
- `Access-Control-Expose-Headers: Content-Length, Content-Range, Accept-Ranges`

---

## 9. ERROR HANDLING & RESILIENCE

### 9.1 File Upload Errors

**Location**: `api/routers/sources.py:save_uploaded_file()` (lines 62-83)

**Error Handling**:
- File write failures logged
- Partial files cleaned up with `os.unlink()`
- Exception re-raised with context

### 9.2 Processing Failures

**Sync Processing** (lines 500-516):
- Check `result.is_success()`
- Clean up source record if failed
- Delete uploaded file
- Return 500 error

**Async Processing**:
- Command fails asynchronously
- Status visible via `/sources/{id}/status`
- Retry available via `POST /sources/{id}/retry`

### 9.3 Chunk Extraction Failures

**Location**: `open_notebook/graphs/source.py` (lines 74-86)

**Error Handling**:
- Chunk extraction failures don't fail whole process
- Warning logged
- Source processing continues
- Source saved without chunks

### 9.4 Chunk Saving Failures

**Location**: `open_notebook/graphs/source.py` (lines 150-154)

**Error Handling**:
- Caught separately
- Logged but doesn't break source saving
- Source continues to save
- Chunks partially/not stored

---

## 10. PERFORMANCE CHARACTERISTICS

### 10.1 File Upload Performance

**Factors**:
- Network bandwidth
- File size
- Unique naming overhead: negligible

**Typical Flow**:
- Small files (<100MB): < 1s upload
- Large files (>1GB): upload time dominated by network

### 10.2 Processing Performance

**Content Extraction**:
- CPU-only (docling): 5-30s per document
- GPU (docling_gpu): 0.5-3s per document (8-14x speedup)

**Chunk Extraction**:
- Added 1-3s overhead (re-processes document)

**Vectorization**:
- Depends on content size
- Full source + chunks: 5-30s

**Total Sync Processing**:
- Small PDFs (1-5 pages): 10-20s
- Large PDFs (100+ pages): 30-120s

### 10.3 Async Processing

**Latency**:
- Immediate return to user
- Processing happens in background
- Status checkable via API

---

## 11. KEY DESIGN DECISIONS

### 11.1 Separation of Concerns

**Content vs Chunks**:
- Content extraction: Full text via docling/content-core
- Chunk extraction: Spatial data via re-processing
- Rationale: Different use cases (search vs visualization)

### 11.2 Idempotent Chunk Saving

**Process**:
1. DELETE old chunks for source
2. INSERT new chunks
3. Consequence: Safe to re-run without duplication

### 11.3 Async-First Design

**Benefits**:
- UI responsiveness
- Better for large files
- Status tracking for UX

**Fallback**:
- Sync mode for backward compatibility
- Used in tests/scripts

### 11.4 Lazy File Cleanup

**Model**:
- Files kept by default
- Optional auto-delete after processing
- Manual deletion available

**Rationale**:
- Preserves evidence
- Allows re-processing
- User controls lifecycle

### 11.5 Multi-Engine Support

**Architecture**:
- Configuration-driven engine selection
- Easy to add new engines
- content-core abstraction layer

**Extension Points**:
- Add new conditional in `content_process()`
- Add new processor function
- Update ContentSettings enum

---

## 12. FILE MANAGEMENT ARCHITECTURE IMPLICATIONS

### 12.1 Directory Structure
```
./data/
  ├── uploads/                 # User uploaded files
  │   ├── document.pdf
  │   ├── document (1).pdf     # Auto-renamed duplicates
  │   └── ...
  ├── sqlite-db/
  │   └── checkpoints.sqlite   # LangGraph checkpoints
  └── tiktoken-cache/
      └── ...                  # Token counting cache
```

### 12.2 File Organization Considerations

**Current State**:
- All uploads in single flat directory
- No categorization or organization
- Relies on database for metadata

**For Architecture Planning**:
- Add optional subdirectories (by date, notebook, type)?
- Implement soft delete (move to trash folder)?
- Archive processed files to separate location?
- Implement storage quotas per user/notebook?

### 12.3 Cleanup & Lifecycle

**Current Mechanisms**:
- `auto_delete_files` setting (deletes after processing)
- Manual deletion via API
- No soft delete or archive

**Potential Enhancements**:
- Archive old files
- Implement quarantine period
- Batch cleanup operations
- Usage statistics tracking

---

## 13. INTEGRATION POINTS FOR EXTENDING FILE MANAGEMENT

### 13.1 For Custom Directory Structure
```python
# Modify: api/routers/sources.py::save_uploaded_file()
# Add subdirectory logic:
upload_path = Path(UPLOADS_FOLDER) / create_subdirectory(notebook_id)
```

### 13.2 For Storage Quotas
```python
# Add to Source model or ContentSettings:
max_storage_per_notebook: int  # bytes
# Check before upload in create_source()
```

### 13.3 For File Archival
```python
# Add archival command:
@command("archive_source_files", app="open_notebook")
async def archive_old_files():
    # Move old files to archive location
    # Update source.asset.file_path
```

### 13.4 For Cleanup Scheduling
```python
# Add scheduled task:
@command("cleanup_orphaned_files", app="open_notebook")
async def cleanup():
    # Find files without source records
    # Delete or archive
```

---

## SUMMARY TABLE

| Component | Technology | Location | Purpose |
|-----------|-----------|----------|---------|
| File Input | HTML5 FileList API | Frontend Dialog | User selects files |
| Upload | FastAPI multipart/form-data | POST /sources | Save file to disk |
| Storage | Filesystem (./data/uploads/) | Local disk | Persist uploaded files |
| Processing | LangGraph + content-core | source_graph | Extract & chunk content |
| Database | SurrealDB | source, chunk tables | Store metadata & content |
| Monitoring | surreal-commands | Command queue | Track async processing |
| Visualization | PDF.js + bounding boxes | Frontend | Display PDF with chunks |
| Download | FastAPI FileResponse | GET /sources/{id}/download | Allow file downloads |

---

## NEXT STEPS FOR FILE MANAGEMENT PLANNING

1. **Storage Organization**: Design directory structure strategy
2. **Quota Management**: Implement per-user/notebook storage limits
3. **Lifecycle Management**: Soft delete, archive, retention policies
4. **Cleanup Automation**: Scheduled tasks for orphaned files
5. **Access Control**: Ensure proper file permission validation
6. **Monitoring**: Track storage usage, file counts, cleanup operations
