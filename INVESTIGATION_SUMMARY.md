# Architecture Investigation Summary

**Investigator**: Claude Code  
**Date**: November 2025  
**Status**: Complete - Ready for Implementation  
**Scope**: Parser architecture, file processing pipeline, GPU integration, Ollama feasibility

---

## Quick Reference

### System Overview
```
Upload (frontend) 
  â†“ POST /sources â†’ save_uploaded_file()
  â†“ ./data/uploads/ (unique filename)
  â†“ process_source_command() â†’ async job queue
  â†“ source_graph.ainvoke() (LangGraph orchestration)
    â”œâ”€ content_process() â†’ [engine selection & extraction]
    â”œâ”€ save_source() â†’ [chunk persistence]
    â”œâ”€ transform_content() â†’ [optional]
    â””â”€ vectorize() â†’ [optional embeddings]
  â†“ SurrealDB (persistent storage)
    â”œâ”€ source table (full_text, metadata)
    â”œâ”€ chunk table (spatial data, positions)
    â””â”€ source_embedding table (vectors)
```

---

## Critical Findings

### 1. Settings Route Bug Found âš ï¸
**File**: `api/routers/settings.py:39-41`

**Issue**: Type cast missing "docling_gpu" and "docling_granite" support

**Current**:
```python
cast(Literal["auto", "docling", "simple"], ...)  # Missing GPU engines
```

**Fix Required**:
```python
cast(Literal["auto", "docling", "docling_gpu", "docling_granite", "simple"], ...)
```

**Impact**: Cannot save GPU-accelerated settings via API (frontend allows, backend rejects)

**Priority**: HIGH - Fix before production deployment

---

### 2. Parser Integration Points Identified âœ…

**Primary Point**: `open_notebook/graphs/source.py:56-61`
```python
if document_engine == "docling_gpu":
    processed_state = await extract_with_docling_gpu(content_state)
else:
    processed_state = await extract_content(content_state)
```

**To Add Docling-Granite**:
```python
elif document_engine == "docling_granite":
    processed_state = await extract_with_docling_granite(content_state)
```

**Chunk Extraction**: Auto-triggers via `"docling" in extraction_engine.lower()` check

---

### 3. Architecture Strengths ðŸŽ¯

âœ… **Async-first design** - All processing is async/await based  
âœ… **Plugin pattern** - Easy to add new parsers (just async functions)  
âœ… **Configuration-driven** - Runtime engine selection via database settings  
âœ… **Separation of concerns** - Content extraction separate from chunk extraction  
âœ… **Spatial awareness** - Chunks include bounding boxes for PDF visualization  
âœ… **Error resilience** - Chunk extraction failures don't break source saving  
âœ… **Extensible** - Clear integration points for new features

---

### 4. GPU Acceleration Working âœ…

**Current Implementation**:
- Location: `open_notebook/utils/docling_gpu.py`
- CUDA GPU support with AUTO fallback
- Global singleton converter (avoid re-initialization overhead)
- Already integrated into processing pipeline
- Performance: 8-14x faster than CPU

---

## Implementation Roadmap

### Phase 1: Fix Critical Bug (1-2 hours)
1. Update settings route type cast
2. Add docling_gpu and docling_granite
3. Test API accepts all values
4. Verify persistence to database

### Phase 2: Docling-Granite Integration (4-6 hours)
1. Create `open_notebook/processors/docling_granite.py`
2. Update ContentSettings enum
3. Add routing in source.py
4. Update frontend schema and UI
5. Test end-to-end

### Phase 3: Testing & Validation (4-6 hours)
1. Unit tests for new processor
2. Integration tests (upload â†’ parse â†’ chunk)
3. Frontend tests (settings UI)
4. Performance benchmarks

### Phase 4: File Management System (8-12 hours)
1. Directory structure enhancement (by date/notebook)
2. Storage quota system
3. Soft delete & archive
4. Automated cleanup
5. Usage statistics

### Phase 5: Optional - Ollama Integration (5-15 hours)
- Research docling-granite Ollama support
- Design integration approach
- Implement local model serving (if applicable)
- Add to settings UI

**Total Estimated Effort**: 20-30 hours for production-ready implementation

---

## Key Files to Modify

### Backend Files
| File | Change | Reason |
|------|--------|--------|
| `api/routers/settings.py:39-41` | Fix type cast | Critical bug |
| `open_notebook/domain/content_settings.py:10-11` | Add "docling_granite" | Config support |
| `open_notebook/graphs/source.py:56-61` | Add elif for granite | Routing logic |
| `open_notebook/processors/__init__.py` | Export new functions | Module API |
| `open_notebook/processors/docling_granite.py` | Create new processor | Main implementation |

### Frontend Files
| File | Change | Reason |
|------|--------|--------|
| `frontend/src/app/(dashboard)/settings/components/SettingsForm.tsx:18` | Update Zod schema | Validation |
| `frontend/src/app/(dashboard)/settings/components/SettingsForm.tsx:114-119` | Add SelectItem | UI option |
| `frontend/src/app/(dashboard)/settings/components/SettingsForm.tsx:~133` | Update help text | Documentation |

---

## Parser Interface Pattern

All parsers must follow this signature:

```python
async def extract_with_[name](state: ProcessSourceState) -> ProcessSourceState:
    """Extract content using [name] processor."""
    # Implementation
    state.content = extracted_content
    state.metadata["extraction_engine"] = "name"
    return state
```

**Input**: ProcessSourceState with file_path/url/content  
**Output**: ProcessSourceState with content + metadata  
**Pattern**: Async function, idempotent, metadata-aware

---

## Chunk Extraction Behavior

**Auto-Trigger Condition** (source.py:72):
```python
should_extract = ("docling" in extraction_engine.lower() or is_pdf) and file_path
```

**For Docling-Granite**: 
- If metadata["extraction_engine"] = "docling_granite"
- Chunks will auto-extract with spatial data
- No additional code needed (unless granite requires custom extraction)

**Output Format**:
```python
{
    'text': str,
    'order': int,
    'physical_page': int,
    'printed_page': int | None,
    'chapter': str | None,
    'paragraph_number': int | None,
    'element_type': str,  # 'paragraph', 'title', 'table', 'picture'
    'positions': [[page, x1, x2, y1, y2], ...],  # Normalized (0-1)
    'metadata': dict
}
```

---

## Ollama Integration Analysis

### Current State
- No existing Ollama integration
- LLM references in chat/transformation features only
- No local model serving infrastructure

### Integration Options

**Option 1: Direct Integration (Recommended)**
- Use docling-granite PyPI package
- No Ollama dependency
- Simplest path (estimated: 4-6 hours)
- Optimal for document parsing

**Option 2: Ollama-Based Parsing**
- Ollama at localhost:11434
- Custom processor calls Ollama API
- More flexible but complex (estimated: 10-15 hours)
- Better for model swapping

**Option 3: LLM Enhancement (Future)**
- Granite4 for content enhancement (summarization, classification)
- Implement as transformation feature
- Requires Ollama for inference
- Separate from document parsing

### Recommendation
**Start with Option 1** (direct docling-granite integration). Add Ollama-based features later if needed.

---

## Configuration System

### Settings Persistence Flow
```
Frontend SettingsForm 
  â†“ PUT /settings
  â†“ settings.py:update_settings()
  â†“ Cast string to Literal (BUG HERE)
  â†“ ContentSettings.update()
  â†“ SurrealDB
  â†“ Next processing
  â†“ ContentSettings.get_instance()
  â†“ Use selected engine
```

### Current Settings
```python
ContentSettings {
    default_content_processing_engine_doc: "auto" | "docling" | "docling_gpu" | "simple",
    default_content_processing_engine_url: "auto" | "firecrawl" | "jina" | "simple",
    default_embedding_option: "ask" | "always" | "never",
    auto_delete_files: "yes" | "no",
    youtube_preferred_languages: ["en", "pt", ...]
}
```

### After Granite Integration
```python
ContentSettings {
    default_content_processing_engine_doc: "auto" | "docling" | "docling_gpu" | "docling_granite" | "simple",
    # ... rest unchanged
}
```

---

## File Management System

### Current State
- All uploads in flat directory: `./data/uploads/`
- Unique naming prevents overwrites (auto-incrementing counter)
- No directory organization or cleanup automation
- Files persist indefinitely (optional auto-delete setting)

### Enhancement Opportunities

**Directory Organization**:
- By date: `uploads/2025-11/document.pdf`
- By notebook: `uploads/{notebook_id}/document.pdf`
- Hybrid: `uploads/notebooks/{id}/{date}/document.pdf`

**Storage Management**:
- Quota system: `max_storage_per_notebook`
- Soft delete: `deleted_at` timestamp field
- Archive: `archived_at` timestamp field
- Cleanup: Scheduled task for old files

**Monitoring**:
- Usage statistics
- Quota warnings
- Storage trends
- Per-notebook usage

---

## Testing Strategy

### Unit Tests (Priority: High)
- [ ] Processor interface compliance
- [ ] Settings persistence
- [ ] Chunk extraction accuracy
- [ ] Metadata population

### Integration Tests (Priority: High)
- [ ] Upload â†’ Parse â†’ Chunk â†’ Store flow
- [ ] Settings change â†’ Engine routing verification
- [ ] GPU fallback behavior
- [ ] Error handling & recovery

### Frontend Tests (Priority: Medium)
- [ ] Settings form validation
- [ ] API integration
- [ ] UI option rendering
- [ ] Error states

### Performance Tests (Priority: Medium)
- [ ] Baseline vs docling_granite speed
- [ ] GPU memory usage
- [ ] Chunk extraction overhead
- [ ] Upload latency

### End-to-End Tests (Priority: High)
- [ ] Manual upload with docling_granite
- [ ] Verify extraction quality
- [ ] Check chunk bounding boxes
- [ ] PDF viewer functionality
- [ ] Settings persistence
- [ ] Error scenarios

---

## Monitoring & Observability

### Current Logging
- Uses `loguru` logger throughout
- Key points:
  - `source.py:57`: "ðŸš€ Using GPU-accelerated Docling"
  - `docling_gpu.py:76`: "âœ… Document processed with GPU acceleration"
  - `chunk_extractor.py`: Chunk count logged

### Enhancements Needed
- Processing time tracking
- GPU memory usage monitoring
- File operation logging
- Storage quota tracking
- Error rate monitoring

---

## Database Schema Summary

### Source Table
```sql
source {
  id: string,
  asset: { file_path?, url? },
  title?: string,
  topics?: [string],
  full_text?: string,
  command?: RecordID,
  created: string,
  updated: string
}
```

### Chunk Table
```sql
chunk {
  id: string,
  source: RecordID,
  text: string,
  order: int,
  physical_page: int,
  printed_page?: int,
  chapter?: string,
  paragraph_number?: int,
  element_type: string,
  positions: [[float]],
  metadata?: object,
  created: string,
  updated: string
}
```

### Settings (Singleton)
```sql
open_notebook:content_settings {
  default_content_processing_engine_doc: string,
  default_content_processing_engine_url: string,
  default_embedding_option: string,
  auto_delete_files: string,
  youtube_preferred_languages: [string]
}
```

---

## Gotchas & Considerations

### Performance
- âš ï¸ Chunk extraction re-processes PDFs (1-3s overhead)
- âœ… GPU caching reduces initialization overhead
- âš ï¸ Large PDFs (100+ pages) may take 30-120s total

### Security
- âœ… File path validation prevents directory traversal
- âœ… File operations limited to uploads folder
- âš ï¸ No per-user file access control (all users see all files)
- âš ï¸ No file encryption at rest

### Backward Compatibility
- âœ… New engine selection won't break existing documents
- âš ï¸ Settings migration needed for existing users
- âœ… Graceful fallback if engine unavailable

### Scalability
- âš ï¸ Single uploads directory gets crowded
- âš ï¸ No file sharding or distribution
- âœ… Database handles scaling (SurrealDB)
- âš ï¸ GPU memory limited by hardware

---

## Quick Start: For Developers

### To Add New Parser

1. **Create processor module**:
```python
# open_notebook/processors/my_parser.py
async def extract_with_my_parser(state: ProcessSourceState) -> ProcessSourceState:
    # Your implementation
    state.metadata["extraction_engine"] = "my_parser"
    return state
```

2. **Update config**:
```python
# content_settings.py
Literal["auto", "docling", "docling_gpu", "my_parser", "simple"]
```

3. **Add routing**:
```python
# source.py:56-61
elif document_engine == "my_parser":
    processed_state = await extract_with_my_parser(content_state)
```

4. **Update UI**:
```tsx
// SettingsForm.tsx
<SelectItem value="my_parser">My Parser</SelectItem>
```

5. **Test**:
```python
async def test_extract_with_my_parser():
    state = ProcessSourceState(file_path="test.pdf")
    result = await extract_with_my_parser(state)
    assert result.content is not None
```

---

## References & Documentation

### Report Files
- `ARCHITECTURE_INVESTIGATION_REPORT.md` - Detailed technical analysis
- `IMPLEMENTATION_CHECKLIST.md` - Step-by-step implementation guide
- This file: Quick reference summary

### Memory Files (in Serena MCP)
- `file_processing_pipeline_investigation` - Complete file flow documentation
- `parser_architecture_analysis` - Detailed parser architecture
- `current_investigation_summary_nov2025` - Latest findings and bug report

### Key Code Sections
- Engine routing: `source.py:56-61`
- GPU setup: `docling_gpu.py:1-81` and `utils/docling_gpu.py:1-66`
- Chunk extraction: `chunk_extractor.py:1-200+`
- Settings: `content_settings.py` + `settings.py`
- File upload: `sources.py:39-83`

---

## Next Steps

1. **Immediate** (before next sprint):
   - [ ] Review this summary
   - [ ] Verify findings in codebase
   - [ ] Fix settings route bug
   - [ ] Estimate team capacity

2. **Short-term** (next sprint):
   - [ ] Implement docling-granite processor
   - [ ] Comprehensive testing
   - [ ] Deploy to staging
   - [ ] Performance validation

3. **Medium-term** (following sprints):
   - [ ] File management system
   - [ ] Storage quota implementation
   - [ ] Ollama integration (if applicable)
   - [ ] Advanced monitoring

---

## Contacts & Questions

For clarifications on:
- **Architecture**: See ARCHITECTURE_INVESTIGATION_REPORT.md
- **Implementation**: See IMPLEMENTATION_CHECKLIST.md
- **Specific Code**: Check memory files or grep codebase
- **Integration Points**: Refer to "Integration Opportunities" section

---

**Status**: Investigation Complete âœ…  
**Confidence Level**: High (verified all major components)  
**Ready for**: Implementation planning & resource allocation

